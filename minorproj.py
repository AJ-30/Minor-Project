# -*- coding: utf-8 -*-
"""MinorProj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IZnHP1em_ywl3kNUOF2CtR2M3G2k2Xp8

Getting documents belonging to top 10 ctaegories and the list of categories that those docs belong to. It'll be a list because multilabel data is there.
"""

import nltk
nltk.download('reuters')
from nltk.corpus import reuters
cats = reuters.categories()
l = [len(reuters.fileids(cats[i])) for i in range(90)]
l2 = [len(reuters.fileids(cats[j])) for j in range(90)]
l.sort(reverse=True)
categories = [cats[l2.index(l[k])] for k in range(10)]
#categories is the list of top 10 categories

length = len(reuters.fileids())
modids = [0]*length
c=0
for ind in range(length):
    id = reuters.fileids()[ind]
    check=0
    cat = reuters.categories(id)
    for ca in cat:
        if ca in categories:
            check+=1
    if check!=0:
        modids[c]=id
        c=c+1

# c is the number of docs belonging to top 10 ids
Top10catids = [modids[ii] for ii in range(c)] # ids of those c docs
test = [d for d in Top10catids if d.startswith('test/')]#test documents fileids list
train = [d for d in Top10catids if d.startswith('training/')]

x1 = [0]*len(train)
x2 = [0]*len(test)

for x11 in range(len(train)):
    id1 = train[x11]
    l1 = reuters.categories(id1)
    t1=0
    for x12 in l1:
        if x12 in categories:
            t1 = t1+1# number of categories that this doc(id1) belongs to and that are present in top 10 classes
    tt1=[""]*t1
    tin=0
    for x12 in l1:
        if x12 in categories:
            tt1[tin]=x12
            tin+=1
    x1[x11]=tt1

# x1: list of list that contains all categories(out of top 10) that our training docs belong to

for x21 in range(len(test)):
    id2 = test[x21]
    l2 = reuters.categories(id2)
    t2=0
    for x22 in l2:
        if x22 in categories:
            t2 = t2+1# number of categories that this doc(id1) belongs to and that are present in top 10 classes
    tt2=[""]*t2
    tin2=0
    for x22 in l2:
        if x22 in categories:
            tt2[tin2]=x22
            tin2+=1
    x2[x21]=tt2
# x2: list of list that contains all categories(out of top 10) that our test docs belong to
print(length)

"""Feature extraction: Tokenisation and tf-idf scores. Get training and testing feature matrices and label matrices. 

How do we know that our BAG of Words is made from trainng data only??
"""

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from nltk import word_tokenize
from nltk.stem.porter import PorterStemmer
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
Stop_Words = stopwords.words("english")

def tokenize(text):
    clean_txt = re.sub('[^a-z\s]+',' ',text) #replacing spcl chars, punctuations by space
    clean_txt = re.sub('(\s+)',' ',clean_txt) #replacing multiple spaces by single space
    min_length = 3
    words = map(lambda word: word.lower(), word_tokenize(clean_txt)) #tokenizing, lowercase
    words = [word for word in words if word not in Stop_Words] #filtering stopwords
    words = filter(lambda t: len(t)>=min_length, words) #filtering words of length <=2
    tokens =(list(map(lambda token: PorterStemmer().stem(token),words))) #stemming tokens
    return tokens

n_classes = 10
labels = categories
stop_words = stopwords.words("english")

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(tokenizer=tokenize, min_df=3,max_df=0.90, 
                             use_idf=True, sublinear_tf=True,norm='l2')

mlb = MultiLabelBinarizer()
docs = {}
docs['train'] = [reuters.raw(doc_id) for doc_id in train]
docs['test'] = [reuters.raw(doc_id) for doc_id in test]

x_tr = vectorizer.fit_transform(docs['train']).toarray()
x_tst = vectorizer.transform(docs['test']).toarray()
y_tr = mlb.fit_transform(x1)
y_tst = mlb.fit_transform(x2)
featuresswith10cats = vectorizer.get_feature_names()

import pandas as pd
datfram = pd.DataFrame(featuresswith10cats)
xtrain = pd.DataFrame(x_tr)
xtest = pd.DataFrame(x_tst)
xtrain.columns = featuresswith10cats
xtest.columns = featuresswith10cats
ytrain = pd.DataFrame(y_tr)
ytest = pd.DataFrame(y_tst)
ytest.columns=categories
ytrain.columns=categories

# including all features

"""Feature selection on the basis of max mean tfidf of training docs and getting final 1000 features of training and testing docs

*   List item
*   List item
"""

import collections
import itertools
from collections import OrderedDict
all_feats = {}
for i in range(len(xtrain.columns)):
  all_feats[i] = (xtrain.iloc[:,i].sum())/len(train)
allSortf = sorted(all_feats.items(), key=lambda var: var[1],reverse = True)
features1k = collections.OrderedDict(itertools.islice(allSortf, 1000))
train_features1k = xtrain.iloc[:,sorted(list(features1k.keys()))]
test_features1k = xtest.iloc[:,sorted(list(features1k.keys()))]
features1000 = train_features1k.columns
final_1kFeatures = pd.DataFrame(features1000)
test_features1k.head()

#writing to excel files
#final_1kFeatures.to_csv('12nov1kfeatures.csv')
#train_features1k.to_csv('12nov1kTrainMtx.csv')
#test_features1k.to_csv('12nov1kTestMtx.csv')

"""SVM one vs rest classifier"""

from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import LinearSVC
classifier = OneVsRestClassifier(LinearSVC())
classifier.fit(xtrain, y_tr)
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
predictions = classifier.predict(xtest)
f1_macro = f1_score(y_tst, predictions,average='macro')
print(" The macroaveraged F1 score with LinearSVC model is {} ".format(f1_macro*100))

f1_micro = f1_score(y_tst, predictions,average='micro')
print(" The microaveraged f1 with LinearSVC model is {} ".format(f1_micro*100))

"""Neural Network training for 10 binary classifiers one for each class:-"""

import keras
from keras.models import Sequential
from keras.layers import Dense
# training targets:-
y0tr_app = [1-y_tr[u,0] for u in range(len(train))]
d1 = pd.DataFrame(y_tr[:,0])
y0tr_tgt = d1.assign(Negativeclass = y0tr_app)

y1tr_app = [1-y_tr[u1,1] for u1 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,1])
y1tr_tgt = d1.assign(Negativeclass = y1tr_app)

y2tr_app = [1-y_tr[u2,2] for u2 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,2])
y2tr_tgt = d1.assign(Negativeclass = y2tr_app)

y3tr_app = [1-y_tr[u3,3] for u3 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,3])
y3tr_tgt = d1.assign(Negativeclass = y3tr_app)

y4tr_app = [1-y_tr[u4,4] for u4 in range(len(train))]
d4 = pd.DataFrame(y_tr[:,4])
y4tr_tgt = d1.assign(Negativeclass = y4tr_app)

y5tr_app = [1-y_tr[u5,5] for u5 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,5])
y5tr_tgt = d1.assign(Negativeclass = y5tr_app)

y6tr_app = [1-y_tr[u6,6] for u6 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,6])
y6tr_tgt = d1.assign(Negativeclass = y6tr_app)

y7tr_app = [1-y_tr[u7,7] for u7 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,7])
y7tr_tgt = d1.assign(Negativeclass = y7tr_app)

y8tr_app = [1-y_tr[u8,8] for u8 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,8])
y8tr_tgt = d1.assign(Negativeclass = y8tr_app)

y9tr_app = [1-y_tr[u9,9] for u9 in range(len(train))]
d1 = pd.DataFrame(y_tr[:,9])
y9tr_tgt = d1.assign(Negativeclass = y9tr_app)
#test targets:-
y0tst_app = [1-y_tst[v,0] for v in range(len(test))]
d2 = pd.DataFrame(y_tst[:,0])
y0tst_tgt = d2.assign(Negativeclass = y0tst_app)

y1tst_app = [1-y_tst[v1,1] for v1 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,1])
y1tst_tgt = d2.assign(Negativeclass = y1tst_app)

y2tst_app = [1-y_tst[v2,2] for v2 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,2])
y2tst_tgt = d2.assign(Negativeclass = y2tst_app)

y3tst_app = [1-y_tst[v3,3] for v3 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,3])
y3tst_tgt = d2.assign(Negativeclass = y3tst_app)

y4tst_app = [1-y_tst[v4,4] for v4 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,4])
y4tst_tgt = d2.assign(Negativeclass = y4tst_app)

y5tst_app = [1-y_tst[v5,5] for v5 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,5])
y5tst_tgt = d2.assign(Negativeclass = y5tst_app)

y6tst_app = [1-y_tst[v6,6] for v6 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,6])
y6tst_tgt = d2.assign(Negativeclass = y6tst_app)

y7tst_app = [1-y_tst[v7,7] for v7 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,7])
y7tst_tgt = d2.assign(Negativeclass = y7tst_app)

y8tst_app = [1-y_tst[v8,8] for v8 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,8])
y8tst_tgt = d2.assign(Negativeclass = y8tst_app)

y9tst_app = [1-y_tst[v9,9] for v9 in range(len(test))]
d2 = pd.DataFrame(y_tst[:,9])
y9tst_tgt = d2.assign(Negativeclass = y9tst_app)

"""cv error to decide the number of hidden neurons for class 'acq':-"""

import sklearn
from sklearn.model_selection import cross_val_predict
from sklearn.model_selection import cross_val_score
from keras.wrappers.scikit_learn import KerasClassifier

cvs1 = []
nntmp = [20, 30, 50, 70, 90]
for tmp in nntmp:
  def create_netnew():
    netnew = Sequential()
    netnew.add(Dense(tmp, input_shape=(train_features1k.shape[1],), activation='sigmoid'))
    netnew.add(Dense(2, activation='softmax'))
    netnew.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])
    return netnew
  neural_net = KerasClassifier(build_fn = create_netnew, epochs = 20, batch_size = 1, verbose =0)
  cv11 = cross_val_score(neural_net,train_features1k,y1tr_tgt, cv = 3)
  cvs1.append(cv11.mean())
  cvs1
  #obtained cv scores: [0.9864385883803358, 0.986438588380336, 0.9859762675296656, 0.9856680536292187, 0.9865926953305594]
  # fixing number of neurons as 20 which gives cv score as 0.9864385883803358

"""after deciding the number of neurons for each classifier, train:-"""

# For classifier of class 'acq' :-
def create_netnew1():
    netnew = Sequential()
    netnew.add(Dense(20, input_shape=(train_features1k.shape[1],), activation='sigmoid'))
    netnew.add(Dense(2, activation='softmax'))
    netnew.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy',keras.metrics.Precision(), keras.metrics.Recall()])
    return netnew

modelnew1 = create_netnew1()
history = modelnew1.fit(train_features1k, y1tr_tgt, validation_split=0.33, epochs=100, batch_size=1,shuffle = True, verbose=0)
import numpy as np
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['train', 'test'], loc='upper right')
plt.yticks(np.arange(0.95, 1.05, step=0.025))
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.show()
# number of epochs come out as >12 ~ 15
modelnew1.fit(train_features1k, y1tr_tgt,
              batch_size=1,
              epochs=12,
              validation_data=(test_features1k, y1tst_tgt),
              shuffle=True,
              )
lr_probs = modelnew1.predict_proba(test_features1k)
lr_probs = lr_probs[:,0]

from sklearn.metrics import precision_recall_curve

lr_precision, lr_recall, thresholds = precision_recall_curve(y_tst[:,1],lr_probs)
plt.plot(lr_recall, lr_precision, label='Clf1 p-rr curve', linewidth = 0.5)
plt.xlabel('Recall', fontsize = 10)
plt.title('Precision vs Recall for class acq', fontsize = 10)
plt.ylabel('Precision', fontsize = 10)

# table of p,r,thresholds:-
dfclf1 = pd.DataFrame(lr_precision)
dfclf1.columns = ['Precision Clf1']
dfclf1 = dfclf1.assign(Recall = lr_recall)
thresholds = np.append(thresholds, 1.0)
dfclf1 = dfclf1.assign(Threshold = thresholds)

# breakeven point = index = 555. threshold value = 0.551425039768219
yhat1 = [0]*len(test)
for inn in range(len(test)):
    if lr_probs[inn]>0.551425039768219:
        yhat1[inn] = 1

# yhat1 has "predictions of classifier 1" according to brekeven point.
from sklearn.metrics import confusion_matrix
confusion_matrix(y_tst[:,1], yhat1)

# calculation of precision, recall and f measure from this confusion matrix:-
tn1, fp1, fn1, tp1 = confusion_matrix(y_tst[:,1], yhat1).ravel()
precision1 = tp1/(tp1+fp1)#
recall1 = tp1/(tp1+fn1)

"""cv error to decide the number of hidden neurons for 'earn':-"""
